"""
Chat API ì—”ë“œí¬ì¸íŠ¸

AI ëŒ€í™” ê¸°ëŠ¥ (RAG í†µí•©)

ğŸ¯ ì£¼ìš” ê¸°ëŠ¥:
1. RAG ëª¨ë“œ: ë¬¸ì„œ ê²€ìƒ‰ + ë¬¸ì„œ ê¸°ë°˜ ë‹µë³€
2. ì¼ë°˜ ëª¨ë“œ: LLM ì¼ë°˜ ì§€ì‹ ê¸°ë°˜ ë‹µë³€
3. Multi-tenancy: ì¡°ì§/ì‚¬ìš©ìë³„ ë°ì´í„° ê²©ë¦¬
"""
import uuid
import structlog
from fastapi import APIRouter, HTTPException

from src.models.chat import ChatRequest, ChatResponse, Source
from src.core.llm.openai_client import openai_client
from src.core.rag import RAGEngine
from src.utils.logger import get_logger

router = APIRouter()
logger = get_logger(__name__)

# RAG ì—”ì§„ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
# - ì•± ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ ìƒì„±ë˜ì–´ ëª¨ë“  ìš”ì²­ì—ì„œ ì¬ì‚¬ìš©
# - Vector Store ì—°ê²° ë“± ì´ˆê¸°í™” ë¹„ìš© ì ˆì•½
rag_engine = RAGEngine()


@router.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """
    AI ì±„íŒ… (RAG í†µí•©)

    ğŸ¯ ë™ì‘ ë°©ì‹:
    1. RAG ëª¨ë“œ (use_rag=True, ê¸°ë³¸ê°’):
       - ì‚¬ìš©ì ì§ˆë¬¸ â†’ Vector DBì—ì„œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
       - ê²€ìƒ‰ëœ ë¬¸ì„œ + ì§ˆë¬¸ â†’ LLMì— ì „ë‹¬
       - LLMì´ ë¬¸ì„œ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ ìƒì„±
       - ì°¸ê³ í•œ ë¬¸ì„œ(sources) í¬í•¨í•˜ì—¬ ë°˜í™˜

    2. ì¼ë°˜ ëª¨ë“œ (use_rag=False):
       - ì‚¬ìš©ì ì§ˆë¬¸ â†’ ë°”ë¡œ LLMì— ì „ë‹¬
       - LLMì˜ ì¼ë°˜ ì§€ì‹ìœ¼ë¡œ ë‹µë³€

    Args:
        request: ì±„íŒ… ìš”ì²­
            - message: ì‚¬ìš©ì ì§ˆë¬¸
            - organization_id: ì¡°ì§ ID (í•„ìˆ˜)
            - user_id: ì‚¬ìš©ì ID (ì„ íƒ)
            - use_rag: RAG ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ True)

    Returns:
        ChatResponse: AI ì‘ë‹µ
            - message: ë‹µë³€ ë‚´ìš©
            - sources: ì°¸ê³  ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ (RAG ëª¨ë“œì—ë§Œ í¬í•¨)
            - session_id: ì„¸ì…˜ ID

    Raises:
        HTTPException: ì²˜ë¦¬ ì‹¤íŒ¨ ì‹œ

    ğŸ’¡ ì‚¬ìš© ì˜ˆì‹œ:
    ```json
    POST /api/v1/chat
    {
        "message": "í”„ë¡œì íŠ¸ A ë§ˆê°ì¼ì´ ì–¸ì œì•¼?",
        "organization_id": "org_123",
        "user_id": "user_456",
        "use_rag": true
    }
    ```

    Response:
    ```json
    {
        "session_id": "sess_abc123",
        "message": "í”„ë¡œì íŠ¸ Aì˜ ë§ˆê°ì¼ì€ 2024ë…„ 12ì›” 31ì¼ì…ë‹ˆë‹¤.",
        "sources": [
            {
                "text": "í”„ë¡œì íŠ¸ Aì˜ ë§ˆê°ì¼ì€ 2024ë…„ 12ì›” 31ì¼ì…ë‹ˆë‹¤.",
                "score": 0.92,
                "metadata": {"title": "í”„ë¡œì íŠ¸ A ì¼ì •"}
            }
        ],
        "timestamp": "2024-12-01T12:00:00"
    }
    ```
    """
    # ì„¸ì…˜ ID ìƒì„± ë˜ëŠ” ì‚¬ìš©
    session_id = request.session_id or f"sess_{uuid.uuid4().hex[:12]}"

    logger.info(
        "ì±„íŒ… ìš”ì²­ ìˆ˜ì‹ ",
        session_id=session_id,
        message_length=len(request.message),
        organization_id=request.organization_id,
        user_id=request.user_id,
        use_rag=request.use_rag,
    )

    try:
        # RAG ëª¨ë“œ: ë¬¸ì„œ ê²€ìƒ‰ + ë¬¸ì„œ ê¸°ë°˜ ë‹µë³€
        if request.use_rag:
            logger.info("RAG ëª¨ë“œë¡œ ë‹µë³€ ìƒì„± ì¤‘...")

            # RAG ì—”ì§„ìœ¼ë¡œ ë‹µë³€ ìƒì„±
            # - ë‚´ë¶€ì ìœ¼ë¡œ: ë¬¸ì„œ ê²€ìƒ‰ â†’ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± â†’ LLM ë‹µë³€ ìƒì„±
            rag_result = rag_engine.generate_answer(
                query=request.message,
                organization_id=request.organization_id,
                user_id=request.user_id,
            )

            # Source ëª¨ë¸ë¡œ ë³€í™˜
            sources = [
                Source(
                    text=src["text"],
                    score=src["score"],
                    metadata=src["metadata"],
                )
                for src in rag_result["sources"]
            ]

            logger.info(
                "RAG ë‹µë³€ ìƒì„± ì™„ë£Œ",
                session_id=session_id,
                answer_length=len(rag_result["answer"]),
                sources_count=len(sources),
            )

            return ChatResponse(
                session_id=session_id,
                message=rag_result["answer"],
                sources=sources if sources else None,
                suggestions=[
                    "ê´€ë ¨ ë¬¸ì„œ ë” ì°¾ê¸°",
                    "ë‹¤ë¥¸ í”„ë¡œì íŠ¸ ì •ë³´ ê²€ìƒ‰",
                    "ì¼ì • í™•ì¸í•˜ê¸°",
                ],
            )

        # ì¼ë°˜ ëª¨ë“œ: LLM ì¼ë°˜ ì§€ì‹ ê¸°ë°˜ ë‹µë³€
        else:
            logger.info("ì¼ë°˜ LLM ëª¨ë“œë¡œ ë‹µë³€ ìƒì„± ì¤‘...")

            # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
            system_prompt = """ë‹¹ì‹ ì€ Cowexa í˜‘ì—… í”Œë«í¼ì˜ AI ë¹„ì„œì…ë‹ˆë‹¤.

ì‚¬ìš©ìì˜ ì—…ë¬´ë¥¼ ë„ì™€ ìƒì‚°ì„±ì„ ë†’ì´ëŠ” ê²ƒì´ ëª©í‘œì…ë‹ˆë‹¤.

ì§€ì¹¨:
- ì¹œì ˆí•˜ê³  ì „ë¬¸ì ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”
- ëª¨ë¥´ëŠ” ê²ƒì€ ì†”ì§íˆ ë§í•˜ì„¸ìš”
- ê°„ê²°í•˜ë©´ì„œë„ ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”
"""

            # ë©”ì‹œì§€ êµ¬ì„±
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": request.message},
            ]

            # LLM í˜¸ì¶œ
            response_text = await openai_client.generate(messages)

            logger.info(
                "ì¼ë°˜ ë‹µë³€ ìƒì„± ì™„ë£Œ",
                session_id=session_id,
                response_length=len(response_text),
            )

            return ChatResponse(
                session_id=session_id,
                message=response_text,
                sources=None,  # ì¼ë°˜ ëª¨ë“œëŠ” ì°¸ê³  ë¬¸ì„œ ì—†ìŒ
                suggestions=[
                    "ë¬¸ì„œ ê²€ìƒ‰í•˜ê¸°",
                    "íƒœìŠ¤í¬ ìƒì„±í•˜ê¸°",
                    "ì¼ì • í™•ì¸í•˜ê¸°",
                ],
            )

    except Exception as e:
        logger.error(
            "ì±„íŒ… ìš”ì²­ ì‹¤íŒ¨",
            session_id=session_id,
            error=str(e),
            exc_info=True,
        )
        raise HTTPException(
            status_code=500, detail=f"ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {str(e)}"
        )
