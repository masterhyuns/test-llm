"""
RAG (Retrieval-Augmented Generation) ì—”ì§„ êµ¬í˜„

ğŸ“š RAGë€?
- Retrieval: ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
- Augmented: ê²€ìƒ‰ëœ ë¬¸ì„œë¡œ ë³´ê°•
- Generation: ë³´ê°•ëœ ì •ë³´ë¡œ ë‹µë³€ ìƒì„±

ğŸ¯ ì™œ RAGë¥¼ ì‚¬ìš©í•˜ë‚˜?
- LLM(ChatGPT ë“±)ì€ í•™ìŠµ ì‹œì ê¹Œì§€ì˜ ì •ë³´ë§Œ ì•Œê³  ìˆìŒ
- ìµœì‹  ì •ë³´, íšŒì‚¬ ë‚´ë¶€ ë¬¸ì„œ ë“±ì€ ëª¨ë¦„
- RAGë¥¼ í†µí•´ í•„ìš”í•œ ì •ë³´ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ì œê³µí•˜ì—¬ ì •í™•í•œ ë‹µë³€ ìƒì„±

ğŸ’¡ RAG ë™ì‘ íë¦„:
1. ì‚¬ìš©ì ì§ˆë¬¸ ì…ë ¥
2. ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë¬¸ì„œ ê²€ìƒ‰ (Vector DBì—ì„œ)
3. ê²€ìƒ‰ëœ ë¬¸ì„œ + ì§ˆë¬¸ì„ LLMì— ì „ë‹¬
4. LLMì´ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ ìƒì„±

ì˜ˆì‹œ:
ì§ˆë¬¸: "í”„ë¡œì íŠ¸ A ë§ˆê°ì¼ì´ ì–¸ì œì•¼?"
â†’ Vector DBì—ì„œ "í”„ë¡œì íŠ¸ A ê´€ë ¨ ë¬¸ì„œ" ê²€ìƒ‰
â†’ ê²€ìƒ‰ ê²°ê³¼: "í”„ë¡œì íŠ¸ Aì˜ ë§ˆê°ì¼ì€ 2024ë…„ 12ì›” 31ì¼ì…ë‹ˆë‹¤."
â†’ LLMì—ê²Œ ì „ë‹¬: "ë‹¤ìŒ ë¬¸ì„œë¥¼ ì°¸ê³ í•´ì„œ ë‹µë³€í•´: [ê²€ìƒ‰ ê²°ê³¼] ì§ˆë¬¸: í”„ë¡œì íŠ¸ A ë§ˆê°ì¼ì´ ì–¸ì œì•¼?"
â†’ LLM ë‹µë³€: "í”„ë¡œì íŠ¸ Aì˜ ë§ˆê°ì¼ì€ 2024ë…„ 12ì›” 31ì¼ì…ë‹ˆë‹¤."
"""

from typing import List, Dict, Any, Optional, Union
from openai import OpenAI

from src.core.rag.opensearch_store import OpenSearchStore
from src.core.rag.qdrant_store import QdrantStore
from src.config.settings import get_settings
from src.utils.logger import get_logger

settings = get_settings()
logger = get_logger(__name__)


class RAGEngine:
    """
    RAG ì—”ì§„ í´ë˜ìŠ¤

    ğŸ¯ ì£¼ìš” ê¸°ëŠ¥:
    1. ë¬¸ì„œ ì¶”ê°€ (Indexing)
    2. ì§ˆë¬¸-ë‹µë³€ (Query)
    3. ë¬¸ì„œ ê²€ìƒ‰ (Search)

    ğŸ”§ êµ¬ì„± ìš”ì†Œ:
    - QdrantStore: Vector DB ê´€ë¦¬ (ë¬¸ì„œ ì €ì¥/ê²€ìƒ‰)
    - OpenAI Client: LLM ë‹µë³€ ìƒì„±
    """

    def __init__(
        self,
        vector_store: Optional[Union[OpenSearchStore, QdrantStore]] = None,
        llm_model: str = "gpt-4o",
        temperature: float = 0.7,
        max_tokens: int = 2000,
        use_opensearch: bool = True,
    ):
        """
        RAG ì—”ì§„ ì´ˆê¸°í™”

        Args:
            vector_store: Vector Store ì¸ìŠ¤í„´ìŠ¤ (ì—†ìœ¼ë©´ ìë™ ìƒì„±)
            llm_model: ì‚¬ìš©í•  LLM ëª¨ë¸
                     - gpt-4o: ìµœì‹  ê³ ì„±ëŠ¥ ëª¨ë¸ (ì¶”ì²œ)
                     - gpt-4o-mini: ë¹ ë¥´ê³  ì €ë ´í•œ ëª¨ë¸
                     - gpt-4-turbo: ê¸´ ì»¨í…ìŠ¤íŠ¸ ì²˜ë¦¬ì— ê°•í•¨
            temperature: ë‹µë³€ì˜ ì°½ì˜ì„± (0~1)
                       - 0.0: í•­ìƒ ê°™ì€ ë‹µë³€ (ì¼ê´€ì„± ë†’ìŒ)
                       - 0.7: ì ì ˆí•œ ì°½ì˜ì„± (ê¸°ë³¸ê°’, ì¶”ì²œ)
                       - 1.0: ë§¤ìš° ì°½ì˜ì  (ì¼ê´€ì„± ë‚®ìŒ)
            max_tokens: ìµœëŒ€ ë‹µë³€ ê¸¸ì´
                      - í† í°: ë‹¨ì–´ì˜ ì‘ì€ ì¡°ê° (í•œê¸€ 1ê¸€ì â‰ˆ 2-3í† í°)
                      - 2000: ì•½ 700-1000ì ì •ë„
            use_opensearch: OpenSearch ì‚¬ìš© ì—¬ë¶€
                          - True: OpenSearch ì‚¬ìš© (ê¸°ë³¸, ê¶Œì¥)
                          - False: Qdrant ì‚¬ìš© (ë ˆê±°ì‹œ)
        """
        # Vector Store ì´ˆê¸°í™”
        # - ë¬¸ì„œë¥¼ ë²¡í„°ë¡œ ì €ì¥í•˜ê³  ê²€ìƒ‰í•˜ëŠ” ì—­í• 
        if vector_store:
            self.vector_store = vector_store
        elif use_opensearch:
            # OpenSearch ì‚¬ìš© (ê¸°ë³¸)
            # - ê¸°ì¡´ í”Œë«í¼ ë°ì´í„° í™œìš©
            # - íƒœê¹… + ë²¡í„° ê²€ìƒ‰ ì§€ì›
            self.vector_store = OpenSearchStore(
                index_name=settings.opensearch_index,
                hosts=[
                    {
                        "host": settings.opensearch_host,
                        "port": settings.opensearch_port,
                    }
                ],
                http_auth=(
                    settings.opensearch_user,
                    settings.opensearch_password,
                ),
                use_ssl=settings.opensearch_use_ssl,
            )
        else:
            # Qdrant ì‚¬ìš© (ë ˆê±°ì‹œ)
            self.vector_store = QdrantStore()

        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        # - LLM API í˜¸ì¶œìš©
        self.openai_client = OpenAI(api_key=settings.openai_api_key)

        # LLM ì„¤ì •
        self.llm_model = llm_model
        self.temperature = temperature
        self.max_tokens = max_tokens

        # Vector Store íƒ€ì… í™•ì¸
        store_type = "OpenSearch" if isinstance(self.vector_store, OpenSearchStore) else "Qdrant"

        logger.info(
            "RAG ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ",
            vector_store=store_type,
            llm_model=llm_model,
            temperature=temperature,
            max_tokens=max_tokens,
        )

    def add_document(
        self,
        text: str,
        metadata: Dict[str, Any],
        organization_id: str,
        user_id: Optional[str] = None,
        tags: Optional[List[str]] = None,
    ) -> str:
        """
        ë¬¸ì„œë¥¼ RAG ì‹œìŠ¤í…œì— ì¶”ê°€

        ğŸ“¥ Indexingì´ë€?
        - ë¬¸ì„œë¥¼ ê²€ìƒ‰ ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥í•˜ëŠ” ê³¼ì •
        - í…ìŠ¤íŠ¸ â†’ ë²¡í„° ë³€í™˜ â†’ Vector DB ì €ì¥

        Args:
            text: ë¬¸ì„œ ë‚´ìš©
            metadata: ë¬¸ì„œ ë©”íƒ€ë°ì´í„°
                    ì˜ˆ: {"title": "í”„ë¡œì íŠ¸ A", "author": "í™ê¸¸ë™"}
            organization_id: ì¡°ì§ ID
            user_id: ì‚¬ìš©ì ID (ì„ íƒ)
            tags: íƒœê·¸ ë¦¬ìŠ¤íŠ¸ (ì„ íƒ, OpenSearch ì‚¬ìš© ì‹œ ìœ ìš©)
                 ì˜ˆ: ["í”„ë¡œì íŠ¸A", "ì¼ì •", "ì¤‘ìš”"]

        Returns:
            ìƒì„±ëœ ë¬¸ì„œ ID

        ğŸ’¡ ì‚¬ìš© ì˜ˆì‹œ:
        ```python
        rag = RAGEngine()
        doc_id = rag.add_document(
            text="í”„ë¡œì íŠ¸ Aì˜ ë§ˆê°ì¼ì€ 2024ë…„ 12ì›” 31ì¼ì…ë‹ˆë‹¤.",
            metadata={
                "title": "í”„ë¡œì íŠ¸ A ì¼ì •",
                "created_at": "2024-12-01",
            },
            organization_id="org_123",
            tags=["í”„ë¡œì íŠ¸A", "ì¼ì •"],  # íƒœê·¸ ì¶”ê°€
        )
        ```
        """
        try:
            logger.info(
                "ë¬¸ì„œ ì¶”ê°€ ì‹œì‘",
                text_length=len(text),
                organization_id=organization_id,
                tags=tags,
            )

            # Vector Storeì— ë¬¸ì„œ ì €ì¥
            # - OpenSearch: embedding + íƒœê·¸ ì €ì¥
            # - Qdrant: embeddingë§Œ ì €ì¥ (íƒœê·¸ ë¬´ì‹œ)
            if isinstance(self.vector_store, OpenSearchStore):
                doc_id = self.vector_store.add_document(
                    text=text,
                    metadata=metadata,
                    organization_id=organization_id,
                    user_id=user_id,
                    tags=tags,
                )
            else:
                # QdrantëŠ” tags íŒŒë¼ë¯¸í„° ë¯¸ì§€ì›
                doc_id = self.vector_store.add_document(
                    text=text,
                    metadata=metadata,
                    organization_id=organization_id,
                    user_id=user_id,
                )

            logger.info("ë¬¸ì„œ ì¶”ê°€ ì™„ë£Œ", doc_id=doc_id)
            return doc_id

        except Exception as e:
            logger.error("ë¬¸ì„œ ì¶”ê°€ ì‹¤íŒ¨", error=str(e))
            raise

    def search_documents(
        self,
        query: str,
        organization_id: str,
        user_id: Optional[str] = None,
        tags: Optional[List[str]] = None,
        limit: int = 5,
    ) -> List[Dict[str, Any]]:
        """
        ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë¬¸ì„œ ê²€ìƒ‰

        ğŸ” Semantic Search (ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰):
        - í‚¤ì›Œë“œê°€ ì •í™•íˆ ì¼ì¹˜í•˜ì§€ ì•Šì•„ë„ ì˜ë¯¸ê°€ ë¹„ìŠ·í•˜ë©´ ê²€ìƒ‰ë¨
        - ì˜ˆ: "ê°•ì•„ì§€" ê²€ìƒ‰ â†’ "ê°œ", "ë°˜ë ¤ë™ë¬¼" ë¬¸ì„œë„ ê²€ìƒ‰

        Args:
            query: ê²€ìƒ‰ ì§ˆë¬¸/í‚¤ì›Œë“œ
            organization_id: ì¡°ì§ ID
            user_id: ì‚¬ìš©ì ID (ì„ íƒ)
            tags: íƒœê·¸ í•„í„° (ì„ íƒ, OpenSearch ì‚¬ìš© ì‹œ ìœ ìš©)
                 ì˜ˆ: ["í”„ë¡œì íŠ¸A"] â†’ í”„ë¡œì íŠ¸A íƒœê·¸ê°€ ìˆëŠ” ë¬¸ì„œë§Œ
            limit: ìµœëŒ€ ê²°ê³¼ ê°œìˆ˜

        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
            [
                {
                    "id": "ë¬¸ì„œ ID",
                    "score": 0.85,  # ìœ ì‚¬ë„
                    "text": "ë¬¸ì„œ ë‚´ìš©",
                    "metadata": {...},
                    "tags": ["íƒœê·¸1", "íƒœê·¸2"],  # OpenSearchë§Œ
                },
                ...
            ]

        ğŸ’¡ ì‚¬ìš© ì˜ˆì‹œ:
        ```python
        rag = RAGEngine()

        # ê¸°ë³¸ ê²€ìƒ‰
        results = rag.search_documents(
            query="í”„ë¡œì íŠ¸ ë§ˆê°ì¼",
            organization_id="org_123",
            limit=3,
        )

        # íƒœê·¸ í•„í„°ë§ (OpenSearch)
        results = rag.search_documents(
            query="ì¼ì • í™•ì¸",
            organization_id="org_123",
            tags=["í”„ë¡œì íŠ¸A"],  # í”„ë¡œì íŠ¸A íƒœê·¸ë§Œ
            limit=5,
        )

        for result in results:
            print(f"{result['score']:.2f} - {result['text']}")
        ```
        """
        try:
            logger.info("ë¬¸ì„œ ê²€ìƒ‰ ì‹œì‘", query=query, tags=tags)

            # Vector Storeì—ì„œ ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰
            # - OpenSearch: íƒœê·¸ í•„í„°ë§ ì§€ì›
            # - Qdrant: íƒœê·¸ ë¬´ì‹œ
            if isinstance(self.vector_store, OpenSearchStore):
                results = self.vector_store.search(
                    query=query,
                    organization_id=organization_id,
                    user_id=user_id,
                    tags=tags,
                    limit=limit,
                )
            else:
                # QdrantëŠ” tags íŒŒë¼ë¯¸í„° ë¯¸ì§€ì›
                results = self.vector_store.search(
                    query=query,
                    organization_id=organization_id,
                    user_id=user_id,
                    limit=limit,
                )

            logger.info("ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ", results_count=len(results))
            return results

        except Exception as e:
            logger.error("ë¬¸ì„œ ê²€ìƒ‰ ì‹¤íŒ¨", error=str(e))
            raise

    def generate_answer(
        self,
        query: str,
        organization_id: str,
        user_id: Optional[str] = None,
        context_limit: int = 5,
        stream: bool = False,
    ) -> Dict[str, Any]:
        """
        RAGë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€ ìƒì„±

        ğŸ¯ ì „ì²´ RAG íŒŒì´í”„ë¼ì¸:
        1. ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë¬¸ì„œ ê²€ìƒ‰ (Vector DB)
        2. ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ì •ë¦¬
        3. í”„ë¡¬í”„íŠ¸ ìƒì„± (ì‹œìŠ¤í…œ ë©”ì‹œì§€ + ì»¨í…ìŠ¤íŠ¸ + ì§ˆë¬¸)
        4. LLMì— ì „ë‹¬í•˜ì—¬ ë‹µë³€ ìƒì„±
        5. ë‹µë³€ + ì°¸ê³  ë¬¸ì„œ ë°˜í™˜

        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            organization_id: ì¡°ì§ ID
            user_id: ì‚¬ìš©ì ID (ì„ íƒ)
            context_limit: ì»¨í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš©í•  ìµœëŒ€ ë¬¸ì„œ ê°œìˆ˜
            stream: ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ (ë¯¸êµ¬í˜„, ì¶”í›„ í™•ì¥)

        Returns:
            {
                "answer": "ìƒì„±ëœ ë‹µë³€",
                "sources": [ì°¸ê³ í•œ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸],
                "model": "ì‚¬ìš©í•œ LLM ëª¨ë¸",
            }

        ğŸ’¡ ì‚¬ìš© ì˜ˆì‹œ:
        ```python
        rag = RAGEngine()

        # ë¨¼ì € ê´€ë ¨ ë¬¸ì„œë“¤ì„ ì¶”ê°€
        rag.add_document(
            text="í”„ë¡œì íŠ¸ Aì˜ ë§ˆê°ì¼ì€ 2024ë…„ 12ì›” 31ì¼ì…ë‹ˆë‹¤.",
            metadata={"title": "í”„ë¡œì íŠ¸ A"},
            organization_id="org_123",
        )

        # ì§ˆë¬¸í•˜ê¸°
        result = rag.generate_answer(
            query="í”„ë¡œì íŠ¸ A ë§ˆê°ì¼ì´ ì–¸ì œì•¼?",
            organization_id="org_123",
        )

        print(result["answer"])
        # â†’ "í”„ë¡œì íŠ¸ Aì˜ ë§ˆê°ì¼ì€ 2024ë…„ 12ì›” 31ì¼ì…ë‹ˆë‹¤."

        print(f"ì°¸ê³  ë¬¸ì„œ: {len(result['sources'])}ê°œ")
        ```

        ğŸ” ì»¨í…ìŠ¤íŠ¸(Context)ë€?
        - LLMì—ê²Œ ì œê³µí•˜ëŠ” ë°°ê²½ ì§€ì‹/ì°¸ê³  ìë£Œ
        - ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì„ ì •ë¦¬í•˜ì—¬ LLMì—ê²Œ ì „ë‹¬
        - LLMì€ ì´ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ ìƒì„±

        ğŸ’° ë¹„ìš©:
        - Vector Search (embedding): $0.00013 / 1K tokens
        - LLM API (gpt-4o): $2.50 / 1M input tokens, $10.00 / 1M output tokens
        - ì˜ˆ: ì§ˆë¬¸ 1ê°œ + ë¬¸ì„œ 3ê°œ (ê° 500ì) + ë‹µë³€ 200ì
          â†’ embedding: $0.0005 + LLM: $0.005 = ì•½ $0.0055
        """
        try:
            logger.info("ë‹µë³€ ìƒì„± ì‹œì‘", query=query)

            # 1ë‹¨ê³„: ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
            # - Vector DBì—ì„œ ì§ˆë¬¸ê³¼ ìœ ì‚¬í•œ ë¬¸ì„œ ì°¾ê¸°
            logger.info("ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘...")
            search_results = self.search_documents(
                query=query,
                organization_id=organization_id,
                user_id=user_id,
                limit=context_limit,
            )

            # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ë¬¸ì„œ ì—†ì´ ë‹µë³€
            if not search_results:
                logger.warning("ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ - ì¼ë°˜ LLM ë‹µë³€ìœ¼ë¡œ ëŒ€ì²´")
                return self._generate_without_context(query)

            # 2ë‹¨ê³„: ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ì •ë¦¬
            # - ì—¬ëŸ¬ ë¬¸ì„œë¥¼ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ í•©ì¹˜ê¸°
            context = self._build_context(search_results)
            logger.info(
                "ì»¨í…ìŠ¤íŠ¸ ìƒì„± ì™„ë£Œ",
                context_length=len(context),
                sources_count=len(search_results),
            )

            # 3ë‹¨ê³„: í”„ë¡¬í”„íŠ¸ ìƒì„±
            # - ì‹œìŠ¤í…œ ë©”ì‹œì§€: AIì˜ ì—­í• ê³¼ í–‰ë™ ì§€ì¹¨
            # - ì‚¬ìš©ì ë©”ì‹œì§€: ì»¨í…ìŠ¤íŠ¸ + ì§ˆë¬¸
            messages = [
                {
                    "role": "system",
                    "content": self._get_system_prompt(),
                },
                {
                    "role": "user",
                    "content": self._build_user_prompt(context, query),
                },
            ]

            # 4ë‹¨ê³„: LLM API í˜¸ì¶œí•˜ì—¬ ë‹µë³€ ìƒì„±
            logger.info("LLM ë‹µë³€ ìƒì„± ì¤‘...", model=self.llm_model)
            response = self.openai_client.chat.completions.create(
                model=self.llm_model,
                messages=messages,
                temperature=self.temperature,
                max_tokens=self.max_tokens,
            )

            # 5ë‹¨ê³„: ë‹µë³€ ì¶”ì¶œ
            answer = response.choices[0].message.content

            # 6ë‹¨ê³„: ê²°ê³¼ ì •ë¦¬ ë° ë°˜í™˜
            result = {
                "answer": answer,
                "sources": [
                    {
                        "text": src["text"],
                        "score": src["score"],
                        "metadata": src["metadata"],
                    }
                    for src in search_results
                ],
                "model": self.llm_model,
            }

            logger.info(
                "ë‹µë³€ ìƒì„± ì™„ë£Œ",
                answer_length=len(answer),
                sources_count=len(search_results),
            )

            return result

        except Exception as e:
            logger.error("ë‹µë³€ ìƒì„± ì‹¤íŒ¨", error=str(e))
            raise

    def _build_context(self, search_results: List[Dict[str, Any]]) -> str:
        """
        ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´ë¡œ ë³€í™˜

        ğŸ“ ì»¨í…ìŠ¤íŠ¸ í˜•ì‹:
        ```
        [ë¬¸ì„œ 1] (ìœ ì‚¬ë„: 0.92)
        í”„ë¡œì íŠ¸ Aì˜ ë§ˆê°ì¼ì€ 2024ë…„ 12ì›” 31ì¼ì…ë‹ˆë‹¤.

        [ë¬¸ì„œ 2] (ìœ ì‚¬ë„: 0.85)
        í”„ë¡œì íŠ¸ A ë‹´ë‹¹ìëŠ” í™ê¸¸ë™ì…ë‹ˆë‹¤.
        ```

        Args:
            search_results: Vector DB ê²€ìƒ‰ ê²°ê³¼

        Returns:
            ì •ë¦¬ëœ ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´
        """
        context_parts = []

        for i, result in enumerate(search_results, 1):
            # ê° ë¬¸ì„œë¥¼ "[ë¬¸ì„œ N] (ìœ ì‚¬ë„: 0.XX)" í˜•ì‹ìœ¼ë¡œ ì¶”ê°€
            score = result.get("score", 0.0)
            text = result.get("text", "")

            context_parts.append(
                f"[ë¬¸ì„œ {i}] (ìœ ì‚¬ë„: {score:.2f})\n{text}"
            )

        # ë¬¸ì„œë“¤ì„ ë¹ˆ ì¤„ë¡œ êµ¬ë¶„í•˜ì—¬ í•©ì¹˜ê¸°
        return "\n\n".join(context_parts)

    def _get_system_prompt(self) -> str:
        """
        ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±

        ğŸ­ System Promptë€?
        - AIì˜ ì—­í• ê³¼ í–‰ë™ ë°©ì‹ì„ ì •ì˜í•˜ëŠ” ì§€ì¹¨
        - ë‹µë³€ ìŠ¤íƒ€ì¼, ì œì•½ ì‚¬í•­ ë“±ì„ ëª…ì‹œ
        - ëª¨ë“  ëŒ€í™”ì— ì¼ê´€ë˜ê²Œ ì ìš©ë¨

        Returns:
            ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´

        ğŸ’¡ í”„ë¡¬í”„íŠ¸ êµ¬ì„± ìš”ì†Œ:
        - ì—­í•  ì •ì˜: "ë‹¹ì‹ ì€ í˜‘ì—… í”Œë«í¼ì˜ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤"
        - í–‰ë™ ì§€ì¹¨: "ì œê³µëœ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”"
        - ì œì•½ ì‚¬í•­: "ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³  ë§í•˜ì„¸ìš”"
        """
        return """ë‹¹ì‹ ì€ í˜‘ì—… í”Œë«í¼ Cowexaì˜ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

ì—­í• :
- ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€í•©ë‹ˆë‹¤
- ì œê³µëœ ë¬¸ì„œ(ì»¨í…ìŠ¤íŠ¸)ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•©ë‹ˆë‹¤
- ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤

ë‹µë³€ ì§€ì¹¨:
1. ì œê³µëœ ë¬¸ì„œì˜ ë‚´ìš©ì„ ìš°ì„ ì ìœ¼ë¡œ ì‚¬ìš©í•˜ì„¸ìš”
2. ë¬¸ì„œì— ì •ë³´ê°€ ì—†ìœ¼ë©´ "ì œê³µëœ ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µë³€í•˜ì„¸ìš”
3. ë‹µë³€ì€ ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”
4. í•„ìš”ì‹œ ë¬¸ì„œ ë²ˆí˜¸ë¥¼ ì¸ìš©í•˜ì—¬ ì¶œì²˜ë¥¼ ëª…ì‹œí•˜ì„¸ìš” (ì˜ˆ: [ë¬¸ì„œ 1]ì— ë”°ë¥´ë©´...)

ì£¼ì˜ì‚¬í•­:
- ê°œì¸ì •ë³´ë‚˜ ë¯¼ê°í•œ ì •ë³´ëŠ” ì‹ ì¤‘í•˜ê²Œ ë‹¤ë£¨ì„¸ìš”
- í™•ì‹¤í•˜ì§€ ì•Šì€ ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”
- í•­ìƒ ì¹œì ˆí•˜ê³  ì „ë¬¸ì ì¸ í†¤ì„ ìœ ì§€í•˜ì„¸ìš”
"""

    def _build_user_prompt(self, context: str, query: str) -> str:
        """
        ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ ìƒì„±

        ğŸ“ User Prompt êµ¬ì¡°:
        ```
        ë‹¤ìŒì€ ì°¸ê³ í•  ë¬¸ì„œë“¤ì…ë‹ˆë‹¤:

        [ë¬¸ì„œ 1] (ìœ ì‚¬ë„: 0.92)
        ...

        [ë¬¸ì„œ 2] (ìœ ì‚¬ë„: 0.85)
        ...

        ì§ˆë¬¸: í”„ë¡œì íŠ¸ A ë§ˆê°ì¼ì´ ì–¸ì œì•¼?
        ```

        Args:
            context: ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì˜ ì»¨í…ìŠ¤íŠ¸
            query: ì‚¬ìš©ì ì§ˆë¬¸

        Returns:
            ì™„ì„±ëœ í”„ë¡¬í”„íŠ¸
        """
        return f"""ë‹¤ìŒì€ ì°¸ê³ í•  ë¬¸ì„œë“¤ì…ë‹ˆë‹¤:

{context}

ì§ˆë¬¸: {query}

ìœ„ ë¬¸ì„œë“¤ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”."""

    def _generate_without_context(self, query: str) -> Dict[str, Any]:
        """
        ì»¨í…ìŠ¤íŠ¸ ì—†ì´ ì¼ë°˜ LLM ë‹µë³€ ìƒì„±

        âš ï¸ ì–¸ì œ ì‚¬ìš©?
        - ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ì„ ë•Œ (ê´€ë ¨ ë¬¸ì„œê°€ DBì— ì—†ìŒ)
        - fallback ë©”ì»¤ë‹ˆì¦˜

        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸

        Returns:
            ë‹µë³€ ê²°ê³¼ (sourcesëŠ” ë¹ˆ ë¦¬ìŠ¤íŠ¸)

        ğŸ’¡ ë™ì‘:
        - Vector DB ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ì–´ë„ LLMì˜ ì¼ë°˜ ì§€ì‹ìœ¼ë¡œ ë‹µë³€
        - ë‹¤ë§Œ, íšŒì‚¬ ë‚´ë¶€ ì •ë³´ëŠ” ë‹µë³€ ë¶ˆê°€
        """
        logger.info("ì»¨í…ìŠ¤íŠ¸ ì—†ì´ ë‹µë³€ ìƒì„±", query=query)

        messages = [
            {
                "role": "system",
                "content": """ë‹¹ì‹ ì€ í˜‘ì—… í”Œë«í¼ Cowexaì˜ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ë˜, ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ì—ˆìŒì„ ì•Œë ¤ì£¼ì„¸ìš”.
ì¼ë°˜ì ì¸ ì •ë³´ëŠ” ì œê³µí•  ìˆ˜ ìˆì§€ë§Œ, íšŒì‚¬ ë‚´ë¶€ ì •ë³´ë‚˜ íŠ¹ì • í”„ë¡œì íŠ¸ ì •ë³´ëŠ” ë¬¸ì„œê°€ í•„ìš”í•©ë‹ˆë‹¤.""",
            },
            {
                "role": "user",
                "content": query,
            },
        ]

        response = self.openai_client.chat.completions.create(
            model=self.llm_model,
            messages=messages,
            temperature=self.temperature,
            max_tokens=self.max_tokens,
        )

        answer = response.choices[0].message.content

        return {
            "answer": answer,
            "sources": [],  # ì°¸ê³  ë¬¸ì„œ ì—†ìŒ
            "model": self.llm_model,
        }

    def delete_document(self, doc_id: str) -> bool:
        """
        ë¬¸ì„œ ì‚­ì œ

        Args:
            doc_id: ì‚­ì œí•  ë¬¸ì„œ ID

        Returns:
            ì‚­ì œ ì„±ê³µ ì—¬ë¶€
        """
        try:
            return self.vector_store.delete_document(doc_id)
        except Exception as e:
            logger.error("ë¬¸ì„œ ì‚­ì œ ì‹¤íŒ¨", error=str(e))
            return False

    def get_stats(self) -> Dict[str, Any]:
        """
        RAG ì‹œìŠ¤í…œ í†µê³„ ì¡°íšŒ

        Returns:
            {
                "total_documents": 1234,  # ì´ ë¬¸ì„œ ìˆ˜
                "vector_store": {...},     # Vector Store ì •ë³´
                "llm_model": "gpt-4o",     # ì‚¬ìš© ì¤‘ì¸ LLM ëª¨ë¸
            }
        """
        try:
            # OpenSearchì™€ Qdrantì—ì„œ ì„œë¡œ ë‹¤ë¥¸ ë©”ì„œë“œ ì‚¬ìš©
            if isinstance(self.vector_store, OpenSearchStore):
                vector_store_info = self.vector_store.get_index_stats()
                total_docs = vector_store_info.get("document_count", 0)
            else:
                vector_store_info = self.vector_store.get_collection_info()
                total_docs = vector_store_info.get("vectors_count", 0)

            return {
                "total_documents": total_docs,
                "vector_store": vector_store_info,
                "llm_model": self.llm_model,
                "temperature": self.temperature,
                "max_tokens": self.max_tokens,
            }
        except Exception as e:
            logger.error("í†µê³„ ì¡°íšŒ ì‹¤íŒ¨", error=str(e))
            raise
